{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:a69b6f6ad1b35dddab4f2f7e4aa0a835d025dc0d1f2e9d55291df9f4c2088409"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Introduction to (Py)Spark\n",
      "\n",
      "* Tim Hopper\n",
      "* http://www.twitter.com/tdhopper\n",
      "* Raleigh, NC\n",
      "* Developer and Anecdote Scientist"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Introduction to (Py)Spark\n",
      "\n",
      "> Apache Spark\u2122 is a fast and general engine for large-scale data processing."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "> It provides high-level APIs in Java, Scala and Python, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming. \n",
      "\n",
      "[source](http://spark.apache.org/docs/latest/)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Sum the squares of the integers from 1 to 10."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).map(lambda x: x**2).sum()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 87,
       "text": [
        "385"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    import nltk.corpus as corpus\n",
      "    stopwords = set(corpus.stopwords.words())\n",
      "except ImportError: \n",
      "    stopwords = []"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Most common words in \"THE DEVELOPMENT OF EMBROIDERY IN AMERICA\"\n",
      "\n",
      "rdd = sc.textFile(\"example.txt\")\n",
      "rdd \\\n",
      "    .flatMap(lambda line: line.split()) \\\n",
      "    .map(lambda word: word.strip().lower()) \\\n",
      "    .filter(lambda word: word not in stopwords) \\\n",
      "    .map(lambda word: (word, 1)) \\\n",
      "    .reduceByKey(lambda a, b: a + b) \\\n",
      "    .map(lambda (key, cnt): (cnt, key)) \\\n",
      "    .top(10)\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "[(117, u'upon'),\n",
        " (105, u'embroidery'),\n",
        " (103, u'one'),\n",
        " (101, u'art'),\n",
        " (94, u'work'),\n",
        " (86, u'made'),\n",
        " (84, u'new'),\n",
        " (82, u'project'),\n",
        " (70, u'embroidered'),\n",
        " (62, u'worked')]"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "These examples is running locally on my laptop.\n",
      "\n",
      "The data file (example.txt) is loaded into a _local_ Resilient Distributed Dataset (__RDD__).\n",
      "\n",
      "If my Spark Context (`sc`) were created on a Spark cluster, the data would have be _partitioned_ across the worker nodes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Spark vs Pyspark?\n",
      "\n",
      "Spark is written in Scala. The 'native' API is in Scala.\n",
      "\n",
      "Pyspark is a very lightweight wrapper around the native API. (You can see its implementation [here](https://github.com/apache/spark/tree/master/python/pyspark).)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Key difference:\n",
      "\n",
      "* Python (unlike Scala) is dynamically typed. (RDDs can hold objects of multiple types!)\n",
      "* Pyspark sometimes lags behind Spark in feature releases.\n",
      "\n",
      "(There's also a Java API in case you really hate life.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "![](http://i.imgur.com/YlI8AqEl.png)\n",
      "\n",
      "[source](https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## How is this better than Hadoop?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Major difference:\n",
      "\n",
      "Spark keep data in worker memory while tends to keep data on disk.\n",
      "\n",
      "According to the Spark webpage it can run \"100x faster than Hadoop by exploiting in memory computing and other optimizations \"\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Also: \n",
      "    \n",
      "> RDDs achieve fault tolerance through a notion of lineage: if a partition of an RDD is lost, the RDD has enough information about how it was derived from other RDDs to be able to rebuild just that partition.\n",
      "\n",
      "source: [Spark: Cluster Computing with Working Sets](https://amplab.cs.berkeley.edu/wp-content/uploads/2011/06/Spark-Cluster-Computing-with-Working-Sets.pdf)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Also:\n",
      "    \n",
      "Spark provides 80+ high(er)-level, functional-style operators beyond simple \"map\" and \"reduce\".\n",
      "\n",
      "(Not even to mention Spark SQL, MLib, and GraphX.)\n",
      "\n",
      "For example:\n",
      "\n",
      "* count\n",
      "* countApprox\n",
      "* flatMap\n",
      "* filter\n",
      "* first\n",
      "* flatMap\n",
      "* fold\n",
      "* groupBy\n",
      "* map\n",
      "* reduce\n",
      "* reduceByKey\n",
      "* sample\n",
      "* sortBy\n",
      "* union"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "[pyspark-pictures](http://nbviewer.ipython.org/github/jkthompson/pyspark-pictures/blob/master/pyspark-pictures.ipynb) is a handy help for the Spark API:\n",
      "\n",
      "```\n",
      "rdd1.cartesian(rdd2)\n",
      "```\n",
      "\n",
      "![](http://nbviewer.ipython.org/github/jkthompson/pyspark-pictures/blob/master/images/pyspark-page17.svg)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Installing (Py)Spark locally\n",
      "\n",
      "For Mac users using [Homebrew](http://brew.sh/):\n",
      "\n",
      "```\n",
      "$ brew install apache-spark\n",
      "```\n",
      "\n",
      "Install [Java SDK](http://www.oracle.com/technetwork/java/javase/downloads/index.html)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Launching the Pyspark REPL\n",
      "\n",
      "```\n",
      "$ IPYTHON=1 pyspark\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "You should see:\n",
      "\n",
      "```\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.2.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.6 (default, Sep  9 2014 15:04:36)\n",
      "SparkContext available as sc.\n",
      ">>>\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Launching the Pyspark in an IPython notebook\n",
      "\n",
      "```\n",
      "$ IPYTHON_OPTS=\"notebook --matplotlib inline\" pyspark\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This creates a special IPython notebook that is initialized with a SparkContext object called `sc`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "<pyspark.context.SparkContext at 0x107e9ab10>"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(You can also create [IPython profiles](http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/) [to automate some of this](http://ramhiser.com/2015/02/01/configuring-ipython-notebook-support-for-pyspark/).)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "These commands will start Pyspark in __local__ mode. As opposed to __cluster__ mode."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The exact same code can be run in local and cluster modes! It just depends on how you initialize your Spark session."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Getting Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load a Python iterable into an RDD\n",
      "\n",
      "sc.parallelize(range(10))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "ParallelCollectionRDD[179] at parallelize at PythonRDD.scala:364"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load a text file\n",
      "\n",
      "sc.textFile(\"example.txt\") # Each line is a separate element in the RDD"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 61,
       "text": [
        "example.txt MappedRDD[193] at textFile at null:-1"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load text files\n",
      "\n",
      "sc.textFile(\"example.txt,example2.txt\").collect()[-1001:-991]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 82,
       "text": [
        "[u'least surpass the work sent by the Decorative Art societies of most of',\n",
        " u'our American cities.',\n",
        " u'',\n",
        " u'',\n",
        " u'',\n",
        " u'',\n",
        " u'CHAPTER VII -- AMERICAN TAPESTRY',\n",
        " u'',\n",
        " u'',\n",
        " u'The Society of Decorative Art, has proved itself a means for the']"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "These can be used to load text files from Amazon S3!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "`SparkContext.wholeTextFile`...\n",
      "\n",
      "> ...lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with <code>textFile</code>, which would return one record per line in each file\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`SparkContext.newAPIHadoopRDD`\n",
      "\n",
      "> PySpark can also read any Hadoop InputFormat or write any Hadoop OutputFormat, for both \u2018new\u2019 and \u2018old\u2019 Hadoop MapReduce APIs. \n",
      "\n",
      "For example, [Cassandra](https://github.com/apache/spark/blob/master/examples/src/main/python/cassandra_inputformat.py)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Saving Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`rdd.collect()` converts a RDD object into a Python list on the host machine."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`rdd.saveAsNewAPIHadoopDataset()` saves an RDD object to a Hadoop data source (e.g. HDFS, [Cassandra](https://github.com/Parsely/pyspark-cassandra), )."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Manipulating Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sample data from a high-energy experiment called STAR \n",
      "\n",
      "```\n",
      "Table (on disk) T-s0 (s0) consists of 1 partition with 12 columns and 2173762 rows\n",
      "charge  FLOAT\n",
      "clus    INT\n",
      "dst     INT\n",
      "enumber INT\n",
      "etime   DOUBLE\n",
      "hist    INT\n",
      "nlb     INT\n",
      "qxb     FLOAT\n",
      "rnumber INT\n",
      "tracks  INT\n",
      "vertex  FLOAT\n",
      "zdc     INT\n",
      "266, 909, 159625, 2635, 20000827.0117590018, 159627, 1341, -26.399744, 1239029, 1228, 0.56000537, 48\n",
      "317, 1243, 159625, 2636, 20000827.0117590018, 159627, 1470, -29.07542, 1239029, 1415, 0.4644182, 53\n",
      "281, 1285, 159625, 2637, 20000827.0117590018, 159627, 1663, -6.7535419, 1239029, 1533, 0.53597438, 8\n",
      "204, 1198, 159625, 2638, 20000827.0117590018, 159627, 1806, 11.985353, 1239029, 1123, 0.66897142, 32\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.sql import SQLContext, Row, StructField, StructType, FloatType"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "star_rdd = sc.textFile(\"star2000.csv.gz\").map(lambda l: tuple(map(float, l.split(\",\"))))\n",
      "schema = ['charge', 'clus', 'dst', 'enumber', 'etime', 'hist', 'nlb', 'qxb', 'rnumber', 'tracks', 'vertex', 'zdc']\n",
      "schema = StructType([StructField(n, FloatType(), True) for n in schema])\n",
      "\n",
      "\n",
      "sqlContext = SQLContext(sc)\n",
      "star_schema = sqlContext.applySchema(star_rdd, schema) \n",
      "star_schema.registerTempTable(\"star\")\n",
      "sqlContext.sql(\"SELECT count(col) FROM (SELECT vertex as col FROM star WHERE zdc <= 8) temp\").count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 126,
       "text": [
        "1L"
       ]
      }
     ],
     "prompt_number": 126
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Machine Learning with (Py)Spark"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}